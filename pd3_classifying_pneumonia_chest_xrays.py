# -*- coding: utf-8 -*-
"""PD3-Classifying-Pneumonia-Chest-Xrays.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hS7daGmqtqqUMk-ObAgU4bW-f4ck0WX-

# Group Members
**Samia Ishaque** (1230027)

**Rustem Kakimov** (1220007)

**Laila Ikki** (1097860)

###Loading Data
"""

from pathlib import Path

import numpy as np
from PIL import Image

import matplotlib.pyplot as plt
import seaborn as sns

from google.colab import drive
drive.mount('/content/drive')

# If "Course Project (COMP-5011)" folder doesn't show up in /MyDrive,
# go to "Shared with me" and add a shortcut of the folder to "My Drive"
dataset_zip = Path('/content/drive/MyDrive/Course Project (COMP-5011)/dataset.zip')
print('Dataset found in Drive:', dataset_zip.exists())

# Drastically speed up data access by copying it to Colab machine
!cp '/content/drive/MyDrive/Course Project (COMP-5011)/dataset.zip' /content/

!unzip -q /content/dataset.zip # unzips archive

!du -hs /content/chest_xray # checks folder size

from pathlib import Path

data_dir = Path('/content/chest_xray')

norm_str = 'NORMAL'
pneu_str = 'PNEUMONIA'
virus_str = 'virus'
bacteria_str = 'bacteria'

data = []

for subset in ['train', 'test', 'val']:
    # Read paths of all NORMAL samples (Class 0)
    for path in (data_dir / subset / norm_str).glob('*.jpeg'):
      data.append((path, 0))

    # Read paths of all PNEUMONIA-virus samples (Class 1)
    for path in (data_dir / subset / pneu_str).glob(f'*{virus_str}*.jpeg'):
      data.append((path, 1))

    # Read paths of all PNEUMONIA-bacteria samples (Class 2)
    for path in (data_dir / subset / pneu_str).glob(f'*{bacteria_str}*.jpeg'):
      data.append((path, 2))

data.sort() # sort for consistent train-test-val split between runs, since .glob does not guarantee the same order each time
X_paths, y_labels = zip(*data)

"""###Data Splitting"""

import hashlib
from sklearn.model_selection import train_test_split

# Explicitly specify seed for random_state to make random split repeatable
seed = 31081993

# Percentages of test and validation data sets.
test_perc = 0.15
val_perc = 0.15
# The rest is training data (1 - test_perc - val_perc).

# Split into training data and remaining
X_paths_train, X_paths_test_val, y_train, y_test_val = train_test_split(X_paths, y_labels, test_size=test_perc+val_perc, random_state=seed)

# Split remaining into test and validation data sets
X_paths_test, X_paths_val, y_test, y_val = train_test_split(X_paths_test_val, y_test_val, test_size=val_perc/(test_perc+val_perc), random_state=seed)

# Verify that the train-test-val split is the same across runs by comparing SHA of concatenated paths.
all_paths = ';'.join([','.join([str(p) for p in paths]) for paths in [X_paths_train, X_paths_test, X_paths_val]])
sha = hashlib.sha256(all_paths.encode('utf- 8')).hexdigest()

if sha == 'a37d59b310e101f5771ae70e51b52ae247fead2d640e183313ff11dc722dc70d':
  print('✅ train-test-val split matches previous runs')
else:
  print('⛔️ train-test-val DOES NOT match previous runs!')

# Plots a bar-chart distribution of classes within the dataset.
def plot_class_split():
  labels = ['Normal', 'Viral', 'Bacterial']

  fig, axes = plt.subplots(1, 3, figsize=(15, 4))

  for i, (y, title) in enumerate(zip([y_train, y_test, y_val], ['Training', 'Testing', 'Validation'])):
    normal = list(y).count(0) / len(y)
    viral = list(y).count(1) / len(y)
    bacterial = list(y).count(2) / len(y)
    axes[i].bar(labels, [normal, viral, bacterial], color=['blue', 'orange', 'green'])
    axes[i].set_title(f'{title} data')

  plt.show()

plot_class_split()

"""##Pre-processing"""

# Target size for the input image used in CNN.
target_size = (200, 140)

def preprocess(img_paths):
  images = []
  for path in img_paths:
    with Image.open(path).convert('L') as img:
      img_resized = img.resize(target_size)
      img_mat = np.array(img_resized)
      images.append(img_mat)

  return images

# Plot random 8 images from a given dataset.
def plot_random_img(images):
  fig, axes = plt.subplots(2, 4, figsize=(12, 3))

  indices = np.random.choice(range(len(images)), 8)
  for i, j in enumerate(indices):
    img_np = images[j]

    r = i // 4
    c = i % 4

    # Plot image
    axes[r][c].imshow(img_np, cmap='gray')
    axes[r][c].axis('off')
    axes[r][c].set_title(f'Image {i+1}')

  plt.show()

# Preprocessing training set
train_images = preprocess(X_paths_train)
plot_random_img(train_images)

# Preprocessing testing set
test_images = preprocess(X_paths_test)
plot_random_img(test_images)

# Preprocessing validation set
val_images = preprocess(X_paths_val)
plot_random_img(val_images)

"""###Image Augmentation"""

import tensorflow as tf
import matplotlib.pyplot as plt

normal_count = y_train.count(0)
viral_count = y_train.count(1)
bacterial_count = y_train.count(2)
print('Normal count:', normal_count)
print('Viral count:', viral_count)
print('Bacterial count:', bacterial_count)

missing_normal_count = bacterial_count - normal_count
missing_viral_count = bacterial_count - viral_count
print('Missing Normal count:', missing_normal_count)
print('Missing Viral count:', missing_viral_count)

images_for_augment = []
labels_for_augment = []

for img, label in zip(train_images, y_train):
  # Not enough images of "Normal" class
  if label == 0:
    images_for_augment.append(img)
    labels_for_augment.append(label)
  if len(images_for_augment) >= missing_normal_count:
    break

for img, label in zip(train_images, y_train):
  # Not enough images of "Viral" class
  if label == 1:
    images_for_augment.append(img)
    labels_for_augment.append(label)
  if len(images_for_augment) >= (missing_normal_count+missing_viral_count):
    break

dataset_for_augment = tf.data.Dataset.from_tensor_slices((images_for_augment, labels_for_augment))

def augment_data(image, label):
    image = tf.expand_dims(image, axis=-1)
    image = tf.image.random_flip_left_right(image)
    image = tf.image.random_brightness(image, 0.3)
    image = tf.image.random_contrast(image, 0.4, 0.9)
    image = tf.image.random_jpeg_quality(image, 15, 50)
    image = tf.squeeze(image, axis=-1)
    return image, label

augmented_dataset = dataset_for_augment.map(augment_data)

def plot_images(images, labels):
    plt.figure(figsize=(12, 5))
    for i in range(5):
        plt.subplot(2, 5, i + 1)
        plt.imshow(images[i], cmap='gray')
        plt.title(f"Label: {labels[i]}")
    plt.show()

augmented_images, augmented_labels = zip(*list(augmented_dataset))

augmented_images = tf.stack(augmented_images)
augmented_labels = tf.stack(augmented_labels)

# Plot the original and augmented images
print('BEFORE AUGMENTATION')
plot_images(train_images[:5], y_train[:5])

print('AFTER AUGMENTATION')
plot_images(augmented_images[:5], y_train[:5])

print('Number of augmented_images: ', len(augmented_images))

print(type(train_images))
print(type(y_train))

train_images += list(augmented_images)
y_train += list(augmented_labels)

print(len(train_images), 'should equal' , len(y_train))
print('Normal:', list(y_train).count(0), 'Viral', list(y_train).count(1),'Bacterial', list(y_train).count(2) )
print('Normal:', list(y_test).count(0), 'Viral', list(y_test).count(1),'Bacterial', list(y_test).count(2) )
plot_class_split()

"""##Normalize Images"""

# Normalize images
X_train = np.array(train_images) / 255
X_test = np.array(test_images) / 255
X_val = np.array(val_images) / 255

y_train = np.array(y_train)
y_test = np.array(y_test)
y_val = np.array(y_val)

"""##**CNN**"""

import tensorflow as tf
from keras import datasets, layers, models, regularizers

w, h = target_size

# Instantiate a model
model = models.Sequential()

# 1st Convolutional Layer
model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(h,w,1)))
model.add(layers.BatchNormalization())
model.add(layers.MaxPooling2D((2, 2)))

# 2nd Convolutional Layer
model.add(layers.Conv2D(64, (3,3), activation='relu'))
model.add(layers.Dropout(0.2))
model.add(layers.BatchNormalization())
model.add(layers.MaxPooling2D((2, 2)))

# 3rd Convolutional Layer
model.add(layers.Conv2D(64, (3,3), activation='relu'))
model.add(layers.BatchNormalization())
model.add(layers.MaxPooling2D((2, 2)))

# 4th Convolutional Layer
model.add(layers.Conv2D(128, (3, 3), activation='relu'))
model.add(layers.Dropout(0.2))
model.add(layers.BatchNormalization())
model.add(layers.MaxPooling2D((2, 2)))

# 5th Convolutional Layer
model.add(layers.Conv2D(256, (3, 3), activation='relu'))
model.add(layers.Dropout(0.3))
model.add(layers.BatchNormalization())
model.add(layers.MaxPooling2D((2, 2)))

# Passing it to a Fully Connected layer
model.add(layers.Flatten())

# 1st Fully Connected Layer
model.add(layers.Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.001)))
model.add(layers.Dropout(0.5)) # Add Dropout to prevent overfitting

# 2nd Fully Connected Layer
model.add(layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.001)))
model.add(layers.Dropout(0.5))

# Output Layer
model.add(layers.Dense(3, activation='softmax'))

from keras.callbacks import ReduceLROnPlateau
learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', patience = 2, verbose=1,factor=0.3, min_lr=0.000001)

model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

history = model.fit(X_train, y_train, epochs=20, batch_size=30, validation_data=(X_val, y_val), callbacks = [learning_rate_reduction] )

"""###CNN Analysis"""

# Visualizing the training and validation accuracies
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.xlabel('epochs')
plt.ylabel('accuracy')
plt.legend(['Train', 'Validation'])
plt.show()

from sklearn.metrics import classification_report

y_hat = model.predict(X_test)
predicted_labels = np.argmax(y_hat, axis=1)
print(classification_report(y_test, predicted_labels, target_names = ['Normal', 'Viral', 'Bacterial']))

from sklearn.metrics import confusion_matrix
import pandas as pd

class_labels = ['Normal', 'Viral', 'Bacterial']

def plot_confusion_matrix(actual_labels, predicted_labels):
  c_mat = confusion_matrix(actual_labels, predicted_labels)
  c_mat = pd.DataFrame(c_mat, index=['0','1', '2'], columns=['0','1','2'])

  plt.figure(figsize=(6, 6))
  sns.heatmap(c_mat, fmt='', annot=True, cmap="Blues", xticklabels=class_labels, yticklabels=class_labels)
  plt.ylabel('Actual')
  plt.xlabel('Predicted')

plot_confusion_matrix(y_test, predicted_labels)

"""### CNN Visualization (Feature extraction)"""

conv_layers = []

for layer in model.layers:
  if 'conv' in layer.name:
    conv_layers.append(layer)
    print(layer.name, layer.output.shape)

import math

def plot_cnn_feature_layers(image):
  plot_layer_output(image, 0, 8) # 32 images
  plot_layer_output(image, 1, 8) # 64 images
  plot_layer_output(image, 2, 8) # 64 images
  plot_layer_output(image, 3, 16) # 128 images
  plot_layer_output(image, 4, 16) # 256 images

def plot_layer_output(input_img, layer_index, cols):
  conv_model = tf.keras.Model(inputs=model.inputs, outputs=conv_layers[layer_index].output)

  # Expand dimensions to create an array with one sample
  input_img = np.expand_dims(input_img, axis=0)

  # Get outputs of a convolutional layer
  feature_maps = conv_model.predict(input_img)
  depth = feature_maps.shape[-1]

  rows = math.ceil(depth / cols)
  fig, axes = plt.subplots(rows, cols, figsize=(30, 20))

  for r in range(rows):
    for c in range(cols):
      index = r * cols + c + 1
      axes[r][c].axis('off')
      axes[r][c].imshow(feature_maps[0, :, :, index-1], cmap='gray')

  plt.suptitle(f'Convolutional layer {layer_index + 1}', fontsize=24)
  plt.show()

# Visualize feature layers for a normal case (Class 0)
print('Class:', class_labels[y_test[2]])
plot_cnn_feature_layers(X_test[2])

# Visualize feature layers for a viral pneumonia case (Class 1)
print('Class:', class_labels[y_test[0]])
plot_cnn_feature_layers(X_test[0])

# Visualize feature layers for a bacterial pneumonia case (Class 2)
print('Class:', class_labels[y_test[1]])
plot_cnn_feature_layers(X_test[1])